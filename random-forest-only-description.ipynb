{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c687e3b3-5ed3-4e30-9102-8aad5dc5ad4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rishubh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/rishubh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/rishubh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a52e19c0-227f-4431-93ce-c5f885dfed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "grid = {\n",
    "    \"n_estimators\":np.arange(10,100,10),\n",
    "    \"max_depth\":[None,3,5,10],\n",
    "    \"min_samples_split\":np.arange(2,20,2),\n",
    "    \"min_samples_leaf\":np.arange(1,20,2),\n",
    "    \"max_features\": [0.5,1,\"sqrt\",\"auto\"],\n",
    "    \"max_samples\":[10000,4000,15000,4000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "824ac70a-1878-439c-90a8-3e5d60b4c497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>german &amp; dutch customer services administrator...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>account director berkshire permanent  pr, adve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>desktop support analyst belfast contract hays ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lead ccie consultant gold partner finance the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rgn / rmn nurse uk permanent cvbrowser social ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         train_texts\n",
       "0  german & dutch customer services administrator...\n",
       "1  account director berkshire permanent  pr, adve...\n",
       "2  desktop support analyst belfast contract hays ...\n",
       "3  lead ccie consultant gold partner finance the ...\n",
       "4  rgn / rmn nurse uk permanent cvbrowser social ..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.read_csv('./data/train_texts.csv', header=0, sep=',', quotechar='\"')\n",
    "Y_train = pd.read_csv('./data/train_labels.csv', header=0, sep=',', quotechar='\"')\n",
    "X_test = pd.read_csv('./data/test_texts.csv', header=0, sep=',', quotechar='\"')\n",
    "Y_test = pd.read_csv('./data/test_labels.csv', header=0, sep=',', quotechar='\"')\n",
    "X_train = X_train.head(6000)\n",
    "Y_train = Y_train.head(6000)\n",
    "X_test = X_test.head(2000)\n",
    "Y_test = Y_test.head(2000)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99292c1a-fbec-4bc8-95b7-f3bcf5b5cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    stopword = nltk.corpus.stopwords.words('english')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lower = [word.lower() for word in tokens]\n",
    "    no_stopwords = [word for word in lower if word not in stopword]\n",
    "    no_alpha = [word for word in no_stopwords if word.isalpha()]\n",
    "    lemm_text = [wn.lemmatize(word) for word in no_alpha]\n",
    "    clean_text = lemm_text\n",
    "    counter = Counter(clean_text)\n",
    "    most_occur = counter.most_common(40)\n",
    "    most_occuring_words = [a_tuple[0] for a_tuple in most_occur]\n",
    "    clean_text = [i for i in clean_text if i in most_occuring_words]\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20fca136-d8f0-4e97-bf25-5c36f1ef481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data,tfidf_vect_fit):\n",
    "    X_tfidf = tfidf_vect_fit.transform(data)\n",
    "    words = tfidf_vect_fit.get_feature_names()\n",
    "    X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "    X_tfidf_df.columns = words\n",
    "    return(X_tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1bbfc2f-7910-43eb-8577-4905b15b3163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aar</th>\n",
       "      <th>aarca</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aasl</th>\n",
       "      <th>aat</th>\n",
       "      <th>ab</th>\n",
       "      <th>abacus</th>\n",
       "      <th>abae</th>\n",
       "      <th>abaes</th>\n",
       "      <th>...</th>\n",
       "      <th>zend</th>\n",
       "      <th>zenith</th>\n",
       "      <th>zero</th>\n",
       "      <th>zest</th>\n",
       "      <th>zigbee</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zone</th>\n",
       "      <th>zorba</th>\n",
       "      <th>zouch</th>\n",
       "      <th>ƒx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 11734 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aar  aarca  aaron  aasl  aat   ab  abacus  abae  abaes  ...  zend  \\\n",
       "0  0.0  0.0    0.0    0.0   0.0  0.0  0.0     0.0   0.0    0.0  ...   0.0   \n",
       "1  0.0  0.0    0.0    0.0   0.0  0.0  0.0     0.0   0.0    0.0  ...   0.0   \n",
       "2  0.0  0.0    0.0    0.0   0.0  0.0  0.0     0.0   0.0    0.0  ...   0.0   \n",
       "3  0.0  0.0    0.0    0.0   0.0  0.0  0.0     0.0   0.0    0.0  ...   0.0   \n",
       "4  0.0  0.0    0.0    0.0   0.0  0.0  0.0     0.0   0.0    0.0  ...   0.0   \n",
       "\n",
       "   zenith  zero  zest  zigbee  zoe  zone  zorba  zouch   ƒx  \n",
       "0     0.0   0.0   0.0     0.0  0.0   0.0    0.0    0.0  0.0  \n",
       "1     0.0   0.0   0.0     0.0  0.0   0.0    0.0    0.0  0.0  \n",
       "2     0.0   0.0   0.0     0.0  0.0   0.0    0.0    0.0  0.0  \n",
       "3     0.0   0.0   0.0     0.0  0.0   0.0    0.0    0.0  0.0  \n",
       "4     0.0   0.0   0.0     0.0  0.0   0.0    0.0    0.0  0.0  \n",
       "\n",
       "[5 rows x 11734 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer=clean)\n",
    "tfidf_vect_fit=tfidf_vect.fit(X_train['train_texts'])\n",
    "X_train=vectorize(X_train['train_texts'],tfidf_vect_fit)\n",
    "X_train.head()\n",
    "\n",
    "#tfidf_vect = TfidfVectorizer(analyzer=clean)\n",
    "#tfidf_vect_fit=tfidf_vect.fit(X_test['test_texts'])\n",
    "X_test=vectorize(X_test['test_texts'],tfidf_vect_fit)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2eb5fda-2947-441b-9da6-cfe2b1c0db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomizedSearchCV(\n",
    "RandomForestRegressor(n_jobs=-1,\n",
    "                     random_state=42),\n",
    "                    param_distributions = grid,\n",
    "                     n_iter=5,\n",
    "                    cv=5,\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "803b3840-f95b-42e5-826e-322c49c3c35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=RandomForestRegressor(n_jobs=-1, random_state=42),\n",
       "                   n_iter=5,\n",
       "                   param_distributions={'max_depth': [None, 3, 5, 10],\n",
       "                                        'max_features': [0.5, 1, 'sqrt',\n",
       "                                                         'auto'],\n",
       "                                        'max_samples': [10000, 4000, 15000,\n",
       "                                                        4000],\n",
       "                                        'min_samples_leaf': array([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19]),\n",
       "                                        'min_samples_split': array([ 2,  4,  6,  8, 10, 12, 14, 16, 18]),\n",
       "                                        'n_estimators': array([10, 20, 30, 40, 50, 60, 70, 80, 90])},\n",
       "                   verbose=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee145d74-426c-4d77-a1bc-1874ca94e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0fcd236e-ffba-4006-8b74-f1c2feb124fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_hyp = mean_absolute_error(Y_test,y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8f985ed-7ce7-43fd-ad62-75b30b377a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9332.271056149542"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ed67c-6cef-45c3-9b2b-068e0105793a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0688a8d-1dee-45b3-ba4c-0c7bc4baeb32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
